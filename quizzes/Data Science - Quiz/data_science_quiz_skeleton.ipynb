{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "386bcc53",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "#Importing required packages.\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.tree import DecisionTreeRegressor, export_text, plot_tree\n",
    "from sklearn.metrics import mean_squared_error, r2_score, classification_report,accuracy_score\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", \"is_categorical_dtype\")\n",
    "warnings.filterwarnings(\"ignore\", \"use_inf_as_na\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "170023bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#%%##########################\n",
    "##### Exercise 1 - Data Import ######\n",
    "############################\n",
    "# Import the data\n",
    " = pd.read_csv('')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d359c6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#%%##########################\n",
    "##### Pre-Processing ######\n",
    "###########################\n",
    "#Let's check how the data is distributed\n",
    "#Information about the data columns\n",
    "# ToDo: Insert your code here!\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Insights: ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f37b4861",
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% Print some descriptive statistics for the data frame\n",
    "# ToDo: Insert your code here for describing stats.\n",
    "\n",
    "#%%############################\n",
    "##### Exercise 2 - Data Visualization######\n",
    "#############################\n",
    "# Pairplot for an overview.\n",
    "# ToDo: Insert your code here!\n",
    "\n",
    "\n",
    "\n",
    "# Insights: ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c801eb4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% Create a box plot consisting of quality and alcohol acid\n",
    "# without outliers. \n",
    "# Add title, names to the axes and save the plot as pdf.\n",
    "plt.figure()\n",
    "sns.boxplot(x=, y=, data=)\n",
    "plt.ylabel('Alcohol')\n",
    "plt.xlabel('Quality')\n",
    "plt.title('Boxplot Quality Alcohol')\n",
    "plt.savefig('BoxPlotForWine.pdf')  # alternatively: BoxPlotForWine.png\n",
    "plt.show()\n",
    "# Insights: ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54dbb518",
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% Create a Distplot for the fixed acidity in the data set. \n",
    "# What does the data distribution tell us? \n",
    "plt.figure()\n",
    "sns.displot(a=)\n",
    "plt.xlabel('Fixed Acidity')\n",
    "plt.ylabel('Count')\n",
    "plt.savefig('Distplot.pdf')  # alternatively: Distplot.png\n",
    "plt.show()\n",
    "# Insights: ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "096342de",
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% Create a heatmap with the correlation. Calculate the correlation with \n",
    "# numpy and use the mask option in the heatmap. Add also axis names and titles.\n",
    "# For what is this plot useful? What insight?  \n",
    "corr = wine.corr()\n",
    "mask = np.zeros_like(corr, dtype=bool)\n",
    "mask[np.triu_indices_from(mask)] = True\n",
    "plt.figure()\n",
    "sns.heatmap(corr, mask=mask, annot=True, cbar=True, fmt='.3f', cmap=\"RdBu_r\")\n",
    "plt.title('Correlation Heatmap')\n",
    "plt.tight_layout()\n",
    "plt.savefig('CorrelationHeatmap.pdf')\n",
    "plt.show()\n",
    "# Note: There is a bug in seaborn version 0.12.2 where annotations \n",
    "# (i.e., corr. values) are not plotted. Please use version 0.11 or > 0.13!\n",
    "# Insights: ? \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c56c9439",
   "metadata": {},
   "outputs": [],
   "source": [
    "#%%#########################################\n",
    "##### Exercise 3 - Data Preparation######\n",
    "##########################################\n",
    "\n",
    "# Now it is time to prepare our data for data-modeling\n",
    "# We want to accomplish two tasks with our machine learning models: \n",
    "# 1. A prediction for how much alcohol will be in a wine of certain attributes.\n",
    "# 2. We want to classify if a wine is good or bad for a given set of wine attributes.\n",
    "\n",
    "# Let's have a look at the values for the first prediction:\n",
    "alcohol = wine['alcohol'].value_counts()\n",
    "print(alcohol)\n",
    "# This data looks perfect for our regression task.\n",
    "\n",
    "# Now for the second task: Let's see what values the quality column has.\n",
    "quality = wine['quality'].value_counts()\n",
    "print(quality)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d44086bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% It appears that we only have values between 3 and 8, while 8 being the criterion\n",
    "# for really good wine, and 3 being a really bad wine.\n",
    "\n",
    "# Since we only want to distinguish between good and bad wines, we need to make\n",
    "# bins of values to assign these values to either good or bad category.\n",
    "\n",
    "# We make a binary classificaion for the quality variable.\n",
    "# Doing this, we divide the set of quality measures as good as possible\n",
    "# We do this with the important cut function.\n",
    "# apply the cut function to the respective subset of wine to fill the\n",
    "# wine['quality'] column with the new values.\n",
    "\n",
    "bins = (0, 5.5, 8)\n",
    "group_names = ['bad', 'good']\n",
    "wine['quality_binary'] = pd.cut(...)\n",
    "\n",
    "\n",
    "# Now take a look at the dataframe column quality. Looks better!\n",
    "\n",
    "# Our model only needs 0 and 1 to classify whether the wine is good \n",
    "# or not. Thus, we must assign 0 for bad and 1 for good quality.\n",
    "# We do this with the LabelEncoder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86e8b6e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#%%Now lets assign a label to our quality variable\n",
    "label_quality = LabelEncoder()\n",
    "\n",
    "#Bad becomes 0 and good becomes 1 \n",
    "wine['quality_binary'] = label_quality.fit_transform(wine['quality_binary'])\n",
    "\n",
    "# Count the values of the quality column of wine.\n",
    "print(wine['quality_binary'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4637cde",
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% Make a seaborn countplot for the values of the quality column of wine.\n",
    "plt.figure()\n",
    "sns.countplot(...)\n",
    "plt.tight_layout()\n",
    "plt.savefig('countplot_quality_binary.pdf')\n",
    "plt.show()\n",
    "# Insights: ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36c1f96b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#%%############################################\n",
    "##### Exercise 4 - Data Modeling######\n",
    "##########################################\n",
    "\n",
    "#Preparing the data for the first task\n",
    "\n",
    "# Now seperate the dataset as response variable/ target variable.\n",
    "# We do this by using the drop function for subset of independent variables\n",
    "# and only adding the alcohol column to subset for the dependent/target variable.\n",
    "x = ...\n",
    "y = ...\n",
    "\n",
    "#Train and Test splitting of data \n",
    "x_train, x_test, y_train, y_test = train_test_split(..., ..., test_size=0.2, random_state=42)\n",
    "\n",
    "# Explain the Train-Test split. Why did we split by the test_size?\n",
    "# Please explain why we need a train and a test set.\n",
    "# Purpose/answer: ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a56acc7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% Applying Standard scaling:\n",
    "# Standardization of a dataset is a common requirement for many \n",
    "# machine learning estimators: they might behave badly if the \n",
    "# individual features do not more or less look like standard \n",
    "# normally distributed data (e.g. Gaussian with 0 mean and unit variance).\n",
    "# see: https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html\n",
    "sc = StandardScaler()\n",
    "\n",
    "x_train = sc.fit_transform(x_train)\n",
    "x_test = sc.transform(x_test)\n",
    "# Purpose/answer: ?\n",
    "\n",
    "# Now you have all the data you need to train the model.\n",
    "print('Training sample of scaled independent variables:\\n',  x_train) # training sample (independent variables)\n",
    "print('Training sample of target variable:\\n',  y_train) # training sample (independent variables)\n",
    "print('Test sample of scaled independent variables:\\n',  x_test) # training sample (independent variables)\n",
    "print('Test sample of target variable:\\n',  y_test) # training sample (independent variables)\n",
    "\n",
    "#%% Regression Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31458604",
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% Regression Tree\n",
    "\n",
    "# Please configure the regression tree as learned in the lecture.\n",
    "# You should also describe, why you set certain parameters such as\n",
    "# max_depth and what the effect of these parameters was.\n",
    "\n",
    "regr = ...\n",
    "# Purpose/answer: ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16d4d7f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% Fit regression model\n",
    "regr.fit(..., ...)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bfb094d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% Predict\n",
    "y_pred = ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b144e13",
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% evaluation\n",
    "mse = mean_squared_error(..., ...)\n",
    "r2 = r2_score(..., ...)\n",
    "print('DT: mse = '+ str(mse) + ' r2 = '+ str(r2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9147e2d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% plotting a tree with text\n",
    "\n",
    "sTree = ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "465a00c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% plot tree and save it to pdf\n",
    "plt.figure(figsize=(16,9))\n",
    "plot_tree(..., filled=True, feature_names=list(x.columns), fontsize=9)\n",
    "plt.savefig('tree.pdf')\n",
    "plt.show()\n",
    "\n",
    "# What was the result of your Regression Tree?\n",
    "# Can it efficiently predict the alcohol of the wines?\n",
    "# If not, what could be the problem?\n",
    "# Insights/answer: ?\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb39834f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#%%######################\n",
    "##### Predicting 'quality_binary' (Exercise 4.4 - 4.8) ######\n",
    "#######################\n",
    "\n",
    "##############################################\n",
    "####Preparing the data for the second task####\n",
    "##############################################\n",
    "\n",
    "## Now to our most interesting prediction part: we want to know if we can\n",
    "## classify good and bad wine. Thus, our target variable is the quality.\n",
    "\n",
    "# We seperate the dataset as response variable/ target variable.\n",
    "# We do this by using the drop function for subset of independent variables\n",
    "# and only adding the quality column to subset for the dependent/target variable.\n",
    "x = ...\n",
    "y = ...\n",
    "\n",
    "\n",
    "#Train and Test splitting of data \n",
    "x_train, x_test, y_train, y_test = train_test_split(\n",
    "    ..., ..., test_size=0.2, random_state=42)\n",
    "\n",
    "# Explain the Train-Test split. Why did we split by the test_size?\n",
    "# Please explain why we need a train and a test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf0e661b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% Applying Standard scaling:\n",
    "# Standardization of a dataset is a common requirement for many \n",
    "# machine learning estimators: they might behave badly if the \n",
    "# individual features do not more or less look like standard \n",
    "# normally distributed data (e.g. Gaussian with 0 mean and unit variance).\n",
    "# see: https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html\n",
    "sc = StandardScaler()\n",
    "\n",
    "x_train = sc.fit_transform(...)\n",
    "x_test = sc.transform(...)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f7937fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% Now we are able to make our Random Forest Classifier.\n",
    "#######################\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "rfc = RandomForestClassifier(n_estimators=200, random_state=1337)\n",
    "rfc.fit(..., ...)\n",
    "y_pred = rfc.predict(...)\n",
    "\n",
    "print (\"Train Accuracy: \", accuracy_score(..., ...))\n",
    "print (\"Test Accuracy: \", accuracy_score(..., ...))\n",
    "print(classification_report(y_test, y_pred, target_names=['bad','good']))\n",
    "print(rfc.score(x_test,y_test))\n",
    "\n",
    "# What was the result of your Random Forest Classifier?\n",
    "# Could it efficiently predict the quality of the wines?\n",
    "# If not, what could be the problem?\n",
    "# Insights/answer: ?\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "195c56ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "#%%######################\n",
    "#####Neural Nets ######\n",
    "#######################\n",
    "\n",
    "# Now we try to make a different classifier using a neural net.\n",
    "# We use the same X and y for classification as before.\n",
    "\n",
    "# This neural net structure is given. \n",
    "# Please fill out the missing Parameters in the model.\n",
    "# Please explain the reason and the effect of the parameters.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf0a8719",
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% define hyperparameters and construct model\n",
    "batch_size = ...\n",
    "number_epochs = ...\n",
    "\n",
    "neural_net = MLPClassifier(\n",
    "    hidden_layer_sizes=..., activation='relu', batch_size=...\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52916f0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% train the model with defined batch size, epochs, etc.\n",
    "for ... in range(1, ... + 1):\n",
    "    train_loss = []  # list to collect train loss for each batch\n",
    "    for b_idx in range(batch_size, len(y_train), batch_size):\n",
    "        idx_start = b_idx - batch_size\n",
    "        idx_end = b_idx\n",
    "        # ToDo: define x for each mini batch\n",
    "        # ToDo: define  for each mini batch\n",
    "        neural_net.partial_fit(..., ..., classes=[0, 1])\n",
    "        train_loss.append(neural_net.loss_)\n",
    "\n",
    "    print(f'Loss after {epoch} epochs: {np.mean(train_loss)}')\n",
    "\n",
    "# Insights/answer: ?\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8c42c99",
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% evaluate the model on test set\n",
    "test_results = neural_net.score(..., ...)\n",
    "print(test_results)\n",
    "\n",
    "# Did the model perform better or worse than the Random Forest Classifier?\n",
    "# Please provide reasons for the difference in performance:\n",
    "# Insights/answer: ?\n",
    "\n",
    "    \n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
